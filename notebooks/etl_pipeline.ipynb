{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data Pipeline - ETL with CSV, Pandas, and SQLite\n",
    "\n",
    "<div style=\"background-color: #f2f2f2; padding: 15px; border-radius: 5px; border: 1px solid #dcdcdc;\">\n",
    "    <h3>Submission Details</h3>\n",
    "    <p>‚úçÔ∏è Please fill in your personal data below.</p>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td style=\"width: 200px; font-weight: bold;\">First Name: Iftekher</td>\n",
    "            <td></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"font-weight: bold;\">Last Name: Aziz</td>\n",
    "            <td></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"font-weight: bold;\">Matriculation Number: 12338137</td>\n",
    "            <td></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"font-weight: bold;\">Term and Year</td>\n",
    "            <td>Winter Term 2025</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome! üëã\n",
    "\n",
    "Welcome to your first PLT! In this task, you'll build a complete, albeit simple, data pipeline from scratch. This is a core skill for anyone working with data. You will act like a data engineer, taking raw data, cleaning it up, and preparing it for analysis. Don't worry if some concepts are new; the goal is to learn by doing. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this notebook, you will\n",
    "1. **Extract** data from a CSV file.\n",
    "2. **Transform** the data using Pandas.\n",
    "3. **Load** the transformed data into an SQLite database.\n",
    "4. **Retrieve and Analyze** the data using SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. Data Acquisition (Extraction)\n",
    "Every data journey begins with the raw material: the data itself. In this first section, your task is to act as a data scout. You will find a suitable dataset from a public source, describe its characteristics, and then load it into a Pandas DataFrame, which is the primary tool we will use for the next steps.\n",
    "\n",
    "\n",
    "#### 1.1. Select and Describe the Dataset\n",
    "\n",
    "* **You must find and download a dataset** in CSV format from a reputable public source. Your chosen dataset must meet the following criteria:\n",
    "    * Contain at least **100 rows** and **6 columns**.\n",
    "    * Include at least one column that can be interpreted as a **date or timestamp**.\n",
    "    * Include at least one **categorical column** (text) with more than **5 unique categories**.\n",
    "\n",
    "* You can find a suitable dataset on websites like:\n",
    "    * **Kaggle:** ([https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)) An excellent resource for a wide variety of real-world datasets.\n",
    "    * **Google Dataset Search:** ([https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/)) A search engine specifically for datasets.\n",
    "\n",
    "* **Describe your chosen dataset.** In the space provided below in the notebook, you must explain the columns and their data types. **Crucially, you must also provide a direct link to the public source or download page of the dataset.**\n",
    "\n",
    "* **Example:** *I am using the \"Video Game Sales\" dataset from Kaggle. The columns are: `Rank` (INTEGER), `Name` (TEXT), `Platform` (TEXT), `Year` (INTEGER), `Genre` (TEXT), and `Global_Sales` (REAL). Link: https://www.kaggle.com/datasets/gregorut/videogamesales*\n",
    "\n",
    "* Place the downloaded CSV file in the same directory as your Jupyter Notebook.\n",
    "\n",
    "* **Don't forget to submit your CSV file** on Moodle along with your completed notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"margin-top: 20px\">\n",
    "\n",
    "‚úçÔ∏è <b>Enter your answer here:</b>\n",
    "\n",
    "I am using the <b> ‚ÄúNetflix Movies and TV Shows‚Äù</b> dataset from <b>Kaggle</b>.  \n",
    "This dataset contains information about TV shows and movies available on Netflix up to 2021.  \n",
    "It is suitable because it has <b>8,807 rows</b> and <b>12 columns</b>.  \n",
    "It also includes at least one column that can be interpreted as a <b>date or timestamp</b> ‚Äî namely <code>date_added</code> (Date) and <code>release_year</code> (Timestamp).  \n",
    "Additionally, it contains multiple <b>categorical text columns</b> such as <code>country</code>, <code>listed_in</code>, and <code>rating</code>, each with more than five unique categories.\n",
    "\n",
    "</br>\n",
    "\n",
    "The columns contained here are:  \n",
    "<b>show_id (TEXT)</b>, <b>type (TEXT)</b>, <b>title (TEXT)</b>, <b>director (TEXT)</b>, <b>cast (TEXT)</b>, <b>country (TEXT)</b>, <b>date_added (DATE)</b>, <b>release_year (INTEGER)</b>, <b>rating (TEXT)</b>, <b>duration (TEXT)</b>, <b>listed_in (TEXT)</b>, and <b>description (TEXT)</b>.\n",
    "\n",
    "</br>\n",
    "\n",
    "<b>Link:</b>  \n",
    "<a href=\"https://www.kaggle.com/datasets/shivamb/netflix-shows\" target=\"_blank\">https://www.kaggle.com/datasets/shivamb/netflix-shows</a>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Load your Dataset\n",
    "In this step, you'll load your dataset into a **Pandas DataFrame**. A DataFrame is the single most important tool in data analysis with Python. You can think of it as a smart, programmable spreadsheet, like Excel, that holds your data in rows and columns and allows you to perform powerful operations on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_dataset.csv' with the actual filename.\n",
    "# Note: 'utf-8' is a common encoding. If you get an error, you might need to try\n",
    "# others like 'latin1', 'iso-8859-1', or 'cp1252'.\n",
    "try:\n",
    "    df = pd.read_csv('netflix_titles.csv', encoding='')\n",
    "    print(\"CSV loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: CSV file not found.  Please check the file name and path.\")\n",
    "    # Create an empty DataFrame as a fallback to allow the rest of the notebook to run (for demonstration purposes)\n",
    "    df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: An unexpected error occurred while loading the CSV: {e}\")\n",
    "    df = pd.DataFrame()  # Fallback to an empty DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first few rows of the DataFrame.\n",
    "if not df.empty:\n",
    "    print(\"\\nFirst 5 rows of the DataFrame:\")\n",
    "    display(df.head())\n",
    "\n",
    "    # Examine the structure of the DataFrame (columns, data types, missing values).\n",
    "    print(\"\\nDataFrame Information:\")\n",
    "    df.info()\n",
    "\n",
    "    # Display descriptive statistics.\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    display(df.describe(include='all'))  # Include all data types\n",
    "else:\n",
    "    print(\"\\nDataFrame is empty. Cannot perform initial data exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning and Transformation\n",
    "\n",
    "This is often the most critical part of any data project. Raw data is rarely perfect and needs to be refined before it can be trusted for analysis. In this section, you will act as a data cleaner, inspecting your data for missing values and incorrect types. You will then transform the data by cleaning it up and adding new, calculated information to make it more useful.\n",
    "#### 2.1. Handling Missing Values\n",
    "\n",
    "*   Identify and describe any missing values in your dataset.\n",
    "*   Choose an appropriate strategy to handle them (e.g., drop rows, fill with a specific value, interpolate).\n",
    "*   Explain your reasoning.\n",
    "*   **Example:** *The `price` column has some missing values.  I'll fill these with the median price, as it's less sensitive to outliers than the mean.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"margin-top: 20px\">\n",
    "‚úçÔ∏è <b>Enter your answer here:</b>\n",
    "    \n",
    "In this step, I carefully inspected the dataset to find where values were missing.\n",
    "From the descriptive statistics and the summary (df.isnull().sum()), I found that the columns\n",
    "<b>director</b>, <b>cast</b>, <b>country</b>, <b>rating</b>, <b>duration</b>, and <b>date_added</b> contained missing values.\n",
    "Among them, <i>director</i> and <i>cast</i> had the highest number of missing entries, while about ten rows lacked a <i>date_added</i> value.\n",
    "All other columns, including <i>release_year</i>, <i>type</i>, and <i>title</i>, were complete.\n",
    "\n",
    "\n",
    "<b>Strategy used:</b>\n",
    "‚Ä¢ For missing values in text-based columns (<i>director, cast, country, rating, duration</i>), and <i>date_added</i>, which is a crucial column for time-based analysis, I removed complete row from the dataset.\n",
    "\n",
    "<b>Reasoning:</b>\n",
    "I chose this approach to maintain the dataset‚Äôs size and informational value. After cleaning, the dataset contained no remaining missing values and its total size decreased slightly from 8,807 rows to 5,332 rows. However, it's still a lot of data to analysis\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values.\n",
    "print(\"Missing Values Before Handling:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with any remaining missing values (if applicable and justified).\n",
    "df.dropna(inplace=True) \n",
    "\n",
    "print(\"\\nMissing Values After Handling:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display dataset shape after cleaning\n",
    "print(f\"\\nDataset shape after cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Convert Data Types\n",
    "* Check if the dtypes are appropriate for analysis\n",
    "* If not, change the dtypes. Explain the reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"margin-top: 20px\">\n",
    "‚úçÔ∏è <b>Enter your answer here:</b>\n",
    "\n",
    "After checking the data types, I found that most columns were stored as <b>object</b> type, which in Pandas means they are plain text strings.  \n",
    "This is not ideal for numeric or time-based analysis. Therefore, I verified each column and converted them to more appropriate formats.\n",
    "\n",
    "\n",
    "<b>Data type evaluation and changes:</b><br/>\n",
    "\n",
    "- <code>show_id</code>, <code>title</code>, <code>director</code>, <code>cast</code>, <code>country</code>, <code>duration</code>, <code>listed_in</code>, and <code>description</code> ‚Üí converted to <b>string</b> dtype to handle text consistently and prevent mixed-type issues.  \n",
    "- <code>date_added</code> ‚Üí converted to <b>datetime64[ns]</b> because it represents actual calendar dates. This allows time-based analysis (e.g., content added per year).  \n",
    "- <code>release_year</code> ‚Üí kept as <b>Int64</b> (integer) because it already contains numeric values representing the release year.  \n",
    "- <code>type</code> and <code>rating</code> ‚Üí converted to <b>category</b> dtype because they have a limited number of unique values (e.g., Movie/TV Show, PG/TV-MA). This reduces memory use and speeds up grouping operations.\n",
    "\n",
    "<b>Reason:</b>  \n",
    "These conversions ensure that numeric, temporal, and categorical operations can be performed efficiently and accurately.  \n",
    "After conversion, the dataset contains meaningful, analysis-ready data types instead of generic object fields.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDtypes BEFORE conversion:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert text columns to 'string' dtype\n",
    "text_cols = [\n",
    "    'show_id', 'title', 'director', 'cast', \n",
    "    'country', 'duration', 'listed_in', 'description'\n",
    "]\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].astype('string')\n",
    "\n",
    "# Convert 'date_added' to datetime\n",
    "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce', format='%B %d, %Y')\n",
    "\n",
    "# Ensure 'release_year' is integer (nullable)\n",
    "df['release_year'] = pd.to_numeric(df['release_year'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Convert 'type' to category\n",
    "df['type'] = df['type'].astype('category')\n",
    "\n",
    "# Convert 'rating' to numeric scale\n",
    "# Define a custom mapping\n",
    "rating_mapping = {\n",
    "    'TV-Y': 1,      # Suitable for all children\n",
    "    'TV-Y7': 2,     # Older children\n",
    "    'TV-G': 3,      # General audience\n",
    "    'TV-PG': 4,     # Parental guidance suggested\n",
    "    'PG': 4,\n",
    "    'TV-14': 5,     # Parents strongly cautioned\n",
    "    'PG-13': 5,\n",
    "    'R': 6,         # Restricted\n",
    "    'TV-MA': 7,     # Mature audience only\n",
    "    'NC-17': 8,     # Adults only\n",
    "}\n",
    "\n",
    "# Map rating text to numeric, unmapped values become NaN\n",
    "df['rating_numeric'] = df['rating'].map(rating_mapping)\n",
    "\n",
    "print(\"\\n'rating' column converted to numeric scale (rating_numeric).\")\n",
    "\n",
    "# Show dtypes after conversion\n",
    "print(\"\\nDtypes AFTER conversion:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Preview cleaned dataset\n",
    "display(df.head())\n",
    "\n",
    "# Save the cleaned dataset to CSV\n",
    "output_path = \"netflix_titles_clean.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\n Cleaned dataset has been saved successfully as '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.3. Transform Your Data\n",
    "\n",
    "*   Apply any necessary transformations to your data.\n",
    "*   This might include:\n",
    "    *   Filtering rows based on certain criteria.\n",
    "    *   Adding new calculated columns.\n",
    "    *   Aggregating data (e.g., grouping by a category and calculating sums or averages).\n",
    "    *   Renaming columns for clarity.\n",
    "*   Clearly explain the purpose of each transformation.\n",
    "* Mandatory Transformation: As your final transformation step, you must add a new column to your DataFrame named semester_id. All rows in this column must contain the text value 'WS2025'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"margin-top: 20px\">\n",
    "‚úçÔ∏è <b>Enter your answer here:</b>\n",
    "\n",
    "In this step, I performed several data transformations to prepare the dataset for analysis.  \n",
    "These transformations help ensure the dataset is clean, structured, and suitable for SQL queries and visual analysis.\n",
    "\n",
    "<b>Transformations performed:</b><br/>\n",
    "\n",
    "<b>Removed duplicate and incomplete records:</b>  \n",
    "All duplicate rows were removed based on the <code>show_id</code> column, ensuring each title appears only once.  \n",
    "Then, rows containing empty or missing values were dropped for complete data consistency.\n",
    "\n",
    "<b>Removed unnecessary column:</b>  \n",
    "The <code>rating</code> column was removed because the cleaned version of its numeric counterpart (<code>rating_numeric</code>) is used for analysis.\n",
    "\n",
    "<b>Added calculated column:</b>  \n",
    "Created a new column <code>content_length_type</code> that classifies each title as either a <i>Movie (Minutes)</i> or a <i>TV Show (Seasons)</i> based on its duration text.\n",
    "\n",
    "<b>Renamed columns for clarity:</b>  \n",
    "Renamed <code>listed_in</code> to <code>genre</code> and <code>duration</code> to <code>length_info</code> to make their meaning clearer.\n",
    "\n",
    "<b>Mandatory transformation:</b>  \n",
    "Added a new column named <code>semester_id</code> with a constant text value <b>'WS2025'</b> for all records, as required in the assignment.\n",
    "\n",
    "\n",
    "<b>Additional transformations for analysis:</b><br/>\n",
    "- Aggregated data to find the total number of Movies and TV Shows added per year.  \n",
    "- Filtered Movies from the United States with <code>rating_numeric</code> below 5.0.  \n",
    "- Filtered TV Shows that have more than two seasons.  \n",
    "\n",
    "These transformations enriched the dataset and enabled detailed analysis of Netflix content by type, year, and quality.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"netflix_titles_clean.csv\")\n",
    "\n",
    "# Remove duplicate rows based on 'show_id'\n",
    "df = df.drop_duplicates(subset='show_id')\n",
    "\n",
    "# Replace empty strings or whitespace with NaN, then drop any row containing NaN\n",
    "df = df.replace(r'^\\s*$', pd.NA, regex=True).dropna()\n",
    "\n",
    "# Drop the 'rating' column\n",
    "if 'rating' in df.columns:\n",
    "    df = df.drop(columns=['rating'])\n",
    "\n",
    "# Create a column showing whether it‚Äôs a Movie or TV Show based on duration text\n",
    "df['content_length_type'] = df['duration'].apply(\n",
    "    lambda x: 'Movie (Minutes)' if isinstance(x, str) and 'min' in x\n",
    "    else ('TV Show (Seasons)' if isinstance(x, str) and 'Season' in x else 'Unknown')\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "df.rename(columns={'listed_in': 'genre', 'duration': 'length_info'}, inplace=True)\n",
    "\n",
    "# Mandatory transformation ‚Äî add semester_id\n",
    "df['semester_id'] = 'WS2025'\n",
    "    \n",
    "# Save the updated dataset\n",
    "output_path = \"netflix_titles_transformed.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Updated dataset saved successfully as 'netflix_titles_transformed.csv'.\")\n",
    "\n",
    "df = pd.read_csv(\"netflix_titles_transformed.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie / TV Show Added by Yearly\n",
    "df = pd.read_csv(\"netflix_titles_transformed.csv\")\n",
    "\n",
    "content_summary = (\n",
    "    df.groupby(['release_year', 'type'])\n",
    "      .size()\n",
    "      .reset_index(name='total_added')\n",
    ")\n",
    "print(\"\\nNumber of titles by release year and content type:\")\n",
    "content_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only Movies with rating_numeric below 5.0 in United States\n",
    "df = pd.read_csv(\"netflix_titles_transformed.csv\")\n",
    "\n",
    "# Ensure columns exist\n",
    "if 'type' in df.columns and 'rating_numeric' in df.columns:\n",
    "    # Filter only Movies within rating range 4.0‚Äì6.0\n",
    "    filtered_df = df[\n",
    "        (df['type'] == 'Movie') &\n",
    "        (df['rating_numeric'] < 5.0) & (df['country'] == 'United States')\n",
    "    ]\n",
    "\n",
    "    print(f\"Filtered Movies with numeric rating below 5.0\")\n",
    "    print(f\"Total Movies found: {filtered_df.shape[0]}\")\n",
    "\n",
    "    # Show preview\n",
    "    display(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only TV Shows with more than 2 Season\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"netflix_titles_transformed.csv\")\n",
    "\n",
    "# Ensure the columns exist\n",
    "if 'type' in df.columns and 'length_info' in df.columns:\n",
    "    # Extract the numeric number of seasons from 'length_info'\n",
    "    df['num_seasons'] = df['length_info'].str.extract(r'(\\d+)').astype('float')\n",
    "\n",
    "    # Filter for TV Shows with more than 2 season\n",
    "    filtered_tv = df[\n",
    "        (df['type'] == 'TV Show') &\n",
    "        (df['num_seasons'] > 2)\n",
    "    ]\n",
    "\n",
    "    print(f\"Filtered TV Shows with more than 1 season.\")\n",
    "    print(f\"Total TV Shows found: {filtered_tv.shape[0]}\")\n",
    "\n",
    "    # Show sample of the filtered data\n",
    "    display(filtered_tv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Loading\n",
    "Once your data is cleaned and transformed, you need a permanent and efficient place to store it. Flat files like CSVs are good for starting, but databases are much more powerful for analysis. In this section, you will act as a database administrator, creating a new SQLite database and loading your clean DataFrame into a structured table within it.\n",
    "#### 3.1. Create an SQLite Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Choose a database name.\n",
    "db_name = 'netflix.db'\n",
    "\n",
    "try:\n",
    "    # Connect to the database (it will be created if it doesn't exist).\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    print(f\"Database '{db_name}' created/connected successfully.\")\n",
    "\n",
    "    # Create a cursor object.\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"ERROR: Could not connect to SQLite database: {e}\")\n",
    "    conn = None  # Set conn to None to prevent further database operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Load Data into a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    # Choose a resonable table name.\n",
    "    table_name = 'movies_data' \n",
    "    # Load the DataFrame into the database.\n",
    "\n",
    "    try:\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)  # 'replace' overwrites the table if it exists\n",
    "        print(f\"Data loaded into table '{table_name}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load data into table: {e}\")\n",
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Data Retrieval and Analysis\n",
    "Now that your clean data is securely stored in a database, it's time to put on your data analyst hat. The real value of data comes from the insights you can extract from it. Here, you will use the SQL query language to \"ask questions\" of your data, retrieving specific subsets and performing calculations to uncover patterns.\n",
    "#### 4.1. Write SQL Queries\n",
    "\n",
    "*   Write at least 3 SQL queries to retrieve specific data from the database.\n",
    "*   Demonstrate different types of queries (e.g., selecting specific columns, filtering with WHERE, using aggregate functions, joining tables - if applicable).\n",
    "*   Explain the purpose of each query as a comment.\n",
    "*   The first query must include the verification check \"WHERE semester_id='WS2025'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    \n",
    "    # Query 1: Filter with WHERE clause (Required verification)\n",
    "    query1 = f\"\"\"\n",
    "    SELECT show_id, title, type, release_year, semester_id\n",
    "    FROM {table_name}\n",
    "    WHERE semester_id = 'WS2025' AND type = 'Movie'\n",
    "    LIMIT 5;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result1 = pd.read_sql_query(query1, conn)\n",
    "        print(\"\\n Result of Query 1 (Filter WHERE semester_id = 'WS2025' AND type = 'Movie'):\")\n",
    "        display(result1)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR executing Query 1: {e}\")\n",
    "else:\n",
    "    print(\"\\nCannot perform data retrieval. Database connection failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "\n",
    "    # Query: Select all Movies produced in the United States\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {table_name}\n",
    "    WHERE type = 'Movie' AND country = 'United States'\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        print(\"Result of Query (Movies from United States):\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR executing query: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nCannot perform data retrieval. Database connection failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "\n",
    "    # Query 2: Find Average Numeric Rating by type\n",
    "    query = f\"\"\"\n",
    "    SELECT type,\n",
    "           ROUND(AVG(rating_numeric), 2) AS avg_rating\n",
    "    FROM {table_name}\n",
    "    WHERE rating_numeric IS NOT NULL\n",
    "    GROUP BY type\n",
    "    ORDER BY avg_rating DESC\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        print(\"Result of Query (Average Rating per Type):\")\n",
    "        display(result)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR executing query: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot perform data retrieval. Database connection failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    # Query 4: Find the most common release year for each type (Movie / TV Show)\n",
    "    query = f\"\"\"\n",
    "    SELECT type, release_year, COUNT(*) AS title_count\n",
    "    FROM {table_name}\n",
    "    GROUP BY type, release_year\n",
    "    HAVING COUNT(*) = (\n",
    "        SELECT MAX(title_count)\n",
    "        FROM (\n",
    "            SELECT type AS t, release_year AS r, COUNT(*) AS title_count\n",
    "            FROM {table_name}\n",
    "            GROUP BY type, release_year\n",
    "        ) AS sub\n",
    "        WHERE sub.t = {table_name}.type\n",
    "    )\n",
    "    ORDER BY type;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        print(\"Result of Query (Most Frequent Release Year for Each Type):\")\n",
    "        display(result)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR executing query: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot perform data retrieval. Database connection failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Documentation and Summary\n",
    "\n",
    "Finally, take a moment to reflect on your work. A key skill for a data professional is communicating what you did and what you learned. In the answer box below, please provide a summary addressing the following points:\n",
    "\n",
    "* **Process Summary:** Briefly describe the key steps you took, from finding the CSV to querying the final database.\n",
    "* **Challenges & Solutions:** What was the biggest challenge you faced? (e.g., a tricky data cleaning step, an unexpected error). How did you solve it?\n",
    "* **Potential Improvements:** If you had another week, what would you add or improve in this pipeline? (e.g., more detailed data validation, automated error logging).\n",
    "* **Key Takeaway:** What is the most important thing you learned from completing this assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"margin-top: 20px\">\n",
    "‚úçÔ∏è <b>Enter your answer here:</b>\n",
    "\n",
    "<b>Process Summary:</b><br/>\n",
    "In this project, I created a complete ETL (Extract, Transform, Load) data pipeline using Python, Pandas, and SQLite. I began by selecting and downloading the <b>Netflix Movies and TV Shows</b> dataset from Kaggle. After loading it into a Pandas DataFrame, I performed data cleaning by removing duplicates, handling missing values, and dropping unnecessary columns. I then transformed the dataset by adding new calculated columns such as <code>content_length_type</code> and the mandatory <code>semester_id = 'WS2025'</code>. The cleaned data was loaded into a SQLite database, and I wrote multiple SQL queries to retrieve, filter, and aggregate data ‚Äî including verifying semester entries, counting titles by year, and finding the latest release years for Movies and TV Shows.\n",
    "\n",
    "\n",
    "<b>Challenges & Solutions:</b><br/>\n",
    "The most challenging part was cleaning and transforming the dataset, especially handling missing <code>date_added</code> values and ensuring proper <code>datetime</code> conversion. Some dates were in non-standard text formats (e.g., ‚ÄúSeptember 5, 2019‚Äù), which caused parsing errors. I solved this by specifying the exact format (<code>%B %d, %Y</code>) in <code>pd.to_datetime()</code> and dropping invalid entries. Another challenge was ensuring consistent data types before loading into SQLite ‚Äî I carefully converted text columns to <code>string</code>, numeric columns to <code>Int64</code>, and categories like <code>type</code> and <code>rating</code> to <code>category</code>.\n",
    "\n",
    "\n",
    "<b>Potential Improvements:</b><br/>\n",
    "If I had more time, I would add a validation layer before loading data into the database. For example, I could include automated data quality checks for missing values, invalid years, and duplicate entries. Additionally, I would implement logging functionality to track the ETL process automatically and build visual dashboards (e.g., in Power BI or Matplotlib) to display statistics such as average rating per genre or the number of titles added per year.\n",
    "\n",
    "\n",
    "<b>Key Takeaway:</b><br/>\n",
    "The most important thing I learned from this assignment is how each step of the ETL process builds upon the previous one. Proper data cleaning and transformation are crucial for meaningful analysis. I also gained confidence in combining Python‚Äôs data manipulation power (Pandas) with SQL‚Äôs querying capabilities, which is an essential skill for real-world data engineering and analytics work.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "- [ ] Verify that you have filled out all the required information in all cells. Remember: You must submit a complete, functional, and readable solution that follows the instructions and guidelines. \n",
    "- [ ] Crucial Final Check: Restart the kernel (Kernel -> Restart & Run All) to ensure your notebook runs from top to bottom without errors.\n",
    "- [ ] Please make sure you submit timely. The deadline is always Friday, 13:15, one week after the PLT release.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Files\n",
    "- [ ] Submit this Notebook as *matriculation_number*-plt-1.ipynb, e.g., *05845964-plt-1.ipynb*\n",
    "- [ ] Your csv file\n",
    "- [ ] In case you used any other libraries, submit a requirements.txt file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Reference:**\n",
    "> ChatGPT ([https://chat.openai.com/](https://chat.openai.com/)) was used to assist in writing and formatting the Markdown explanations and code documentation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
